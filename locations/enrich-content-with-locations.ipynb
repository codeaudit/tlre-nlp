{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysolr\n",
    "import spacy\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import dateutil.parser\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') #Use the large model, it works the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deduplicates a list\n",
    "def dedup(arr):\n",
    "    uniques = set(arr)\n",
    "    return list(uniques)\n",
    "\n",
    "#Disambiguate to the nearest lat/lng whole numbers:\n",
    "def near(loc):\n",
    "    latlng = loc[\"latLng\"]\n",
    "    return round(latlng[\"lat\"]),round(latlng[\"lng\"])\n",
    "\n",
    "#Converts a mapquest geocoded location into a normalized City|State|Country string\n",
    "def norm(loc):\n",
    "    loctypes = {\"City\":\"\",\"State\":\"\",\"Country\":\"\"}\n",
    "    location = None\n",
    "    for i in range(8):\n",
    "        aai = \"adminArea\"+str(i)\n",
    "        aat = aai+\"Type\"\n",
    "        if aat in loc.keys() and loc[aat] in loctypes.keys():\n",
    "            loctypes[loc[aat]] = aai\n",
    "    for typ in loctypes.keys():\n",
    "        fld = loctypes[typ]\n",
    "        if fld in loc.keys() and len(loc[fld]):\n",
    "            if not location:\n",
    "                location = loc[fld]\n",
    "            else:\n",
    "                location += \"|\" + loc[fld]\n",
    "    if not location:\n",
    "        location = \"_UNKNOWN_\"\n",
    "    return location.replace(\" \", \"_\")\n",
    "\n",
    "# Converts a mapquest geocoded location into a City, State, Country dictionary\n",
    "def city_state_country(loc):\n",
    "    loctypes = {\"City\":\"\",\"State\":\"\",\"Country\":\"\"}\n",
    "    location = {\"City\":\"\",\"State\":\"\",\"Country\":\"\"}\n",
    "    for i in range(8):\n",
    "        aai = \"adminArea\"+str(i)\n",
    "        aat = aai+\"Type\"\n",
    "        if aat in loc.keys() and loc[aat] in loctypes.keys():\n",
    "            loctypes[loc[aat]] = aai\n",
    "    for typ in loctypes.keys():\n",
    "        fld = loctypes[typ]\n",
    "        if fld in loc.keys() and len(loc[fld]):\n",
    "            location[typ] = loc[fld]\n",
    "    return location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makes requests to the Mapquest service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MapQuest Geocode service.  More info: https://developer.mapquest.com/documentation/geocoding-api/\n",
    "mapquest_address_url=\"https://www.mapquestapi.com/geocoding/v1/address?key=AuqdPFEWYhm7rZRN5hX5HeWSKgaO2u7d&location=\"\n",
    "def geocode(text):\n",
    "    req = requests.get(mapquest_address_url+text)\n",
    "    jsn = req.json()\n",
    "    loc = jsn[\"results\"][0][\"locations\"][0]\n",
    "    return loc\n",
    "\n",
    "# Send batches to the MapQuest Geocode API in up to 100 locations at a time\n",
    "# Saves on API monthly rate (15000 calls per month in the free tier)\n",
    "#mapquest_batch_url=\"http://www.mapquestapi.com/geocoding/v1/batch?&inFormat=json&outFormat=json&key=AuqdPFEWYhm7rZRN5hX5HeWSKgaO2u7d\"\n",
    "def geocode_batch(batch):\n",
    "    mapquest_batch_url=\"http://www.mapquestapi.com/geocoding/v1/batch?key=AuqdPFEWYhm7rZRN5hX5HeWSKgaO2u7d&maxResults=1&thumbMaps=false\"\n",
    "    request_locations = []\n",
    "    reverse_lookup = {}\n",
    "    response = {}\n",
    "    \n",
    "    #Create reverse lookup table for the API response\n",
    "    for movie in batch:\n",
    "        movie_id = movie[\"id\"]\n",
    "        location = movie[\"location\"]\n",
    "        if movie_id not in response:\n",
    "            response[movie_id] = []\n",
    "        if location not in reverse_lookup:\n",
    "            reverse_lookup[location] = []\n",
    "        reverse_lookup[location].append(movie_id)\n",
    "        mapquest_batch_url += \"&location=\" + location\n",
    "    req = requests.get(mapquest_batch_url)\n",
    "    jsn = req.json()\n",
    "\n",
    "    # Map the API location results to the movie IDs\n",
    "    for result in jsn[\"results\"]:\n",
    "        if \"providedLocation\" in result and \"locations\" in result and len(result[\"locations\"]):\n",
    "            provided = result[\"providedLocation\"][\"location\"]\n",
    "            location = result[\"locations\"][0]\n",
    "            if location[\"geocodeQuality\"] != \"COUNTRY\" or location[\"adminArea1\"] != \"US\":\n",
    "                if provided in reverse_lookup:\n",
    "                    for movie_id in reverse_lookup[provided]:\n",
    "                        response[movie_id].append(location)\n",
    "                else:\n",
    "                    print(\"Not found\",provided)\n",
    "                \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get entities with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a basic finite state machine to chunk proper noun GPEs as one location\n",
    "# This had the best F1 score from our tests in the test_location_entities notebook\n",
    "def extract_chunked_locations(text):\n",
    "    doc = nlp(text)\n",
    "    gpes = []\n",
    "    for s in doc.sents:\n",
    "        curr = []\n",
    "        isgpe = False\n",
    "        for t in s:\n",
    "            if (t.pos_ == 'PROPN' and t.ent_type_ == 'GPE') or (isgpe == True and t.text==','):\n",
    "                isgpe = True\n",
    "                if t.text!=',':\n",
    "                    curr.append(t.text)\n",
    "            elif isgpe == True:\n",
    "                gpes.append(' '.join(curr))\n",
    "                curr = []\n",
    "                isgpe = False\n",
    "            else:\n",
    "                isgpe = False\n",
    "        if(len(curr)):\n",
    "            gpes.append(' '.join(curr))\n",
    "            curr = []\n",
    "            isgpe = False\n",
    "    return gpes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Content utlities for the TMDB corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through the movies\n",
    "def rawTmdbMovies(filename):\n",
    "    return json.load(open(filename))\n",
    "\n",
    "\n",
    "def writeTmdmMovies(rawMoviesJson, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(rawMoviesJson, f)\n",
    "\n",
    "def tmdbMovies(filename=\"../tmdb.json\"):\n",
    "    tmdbMovies = rawTmdbMovies(filename)\n",
    "    for movieId, tmdbMovie in tmdbMovies.items():\n",
    "        yield (movieId, tmdbMovie)\n",
    "\n",
    "def indexableMovies(filename=\"../tmdb.json\"):\n",
    "    \"\"\" Generates TMDB movies, similar to how ES Bulk indexing\n",
    "        uses a generator to generate bulk index/update actions \"\"\"\n",
    "    for movieId, tmdbMovie in tmdbMovies(filename):\n",
    "        try:\n",
    "            releaseDate = None\n",
    "            if 'release_date' in tmdbMovie and len(tmdbMovie['release_date']) > 0:\n",
    "                releaseDate = tmdbMovie['release_date'] + 'T00:00:00Z'\n",
    "\n",
    "            yield {'id': movieId,\n",
    "                   'title': tmdbMovie['title'],\n",
    "                   'overview': tmdbMovie['overview'],\n",
    "                   'tagline': tmdbMovie['tagline'],\n",
    "                   'directors': [director['name'] for director in tmdbMovie['directors']],\n",
    "                   'cast': [castMember['name'] for castMember in tmdbMovie['cast']],\n",
    "                   'genres': [genre['name'] for genre in tmdbMovie['genres']],\n",
    "                   'release_date': releaseDate,\n",
    "                   'vote_average': tmdbMovie['vote_average'] if 'vote_average' in tmdbMovie else None,\n",
    "                   'vote_count': int(tmdbMovie['vote_count']) if 'vote_count' in tmdbMovie else None,\n",
    "                   'location_entities': tmdbMovie['location_entities'] if 'location_entities' in tmdbMovie else [],\n",
    "                   'location': tmdbMovie['location'] if 'location' in tmdbMovie else [],\n",
    "                   'location_city': tmdbMovie['location_city'] if 'location_city' in tmdbMovie else [],\n",
    "                   'location_state': tmdbMovie['location_state'] if 'location_state' in tmdbMovie else [],\n",
    "                   'location_country': tmdbMovie['location_country'] if 'location_country' in tmdbMovie else [],\n",
    "                   }\n",
    "        except KeyError as k: # Ignore any movies missing these attributes\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gets all the GPEs in the corpus movies titles and overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all the movies, finds all the entities in titles and overviews, and adds them to the respective movie record\n",
    "def entitize_movies():\n",
    "    tmdb_spacy = {}\n",
    "    for movie_id, movie in tmdbMovies(filename=\"../tmdb.json\"):\n",
    "        title_entities = []\n",
    "        overview_entities = []\n",
    "        if \"title\" in movie and isinstance(movie[\"title\"], str) and len(movie[\"title\"]):\n",
    "            title_entities = extract_chunked_locations(movie[\"title\"])\n",
    "\n",
    "        if \"overview\" in movie and isinstance(movie[\"overview\"], str) and len(movie[\"overview\"]):\n",
    "            overview_entities = extract_chunked_locations(movie[\"overview\"])\n",
    "\n",
    "        movie[\"location_entities\"] = dedup(title_entities + overview_entities)\n",
    "        tmdb_spacy[movie_id] = movie\n",
    "    writeTmdmMovies(tmdb_spacy,'../tmdb_spacy.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocodes all the GPEs for all movies in batches, and enriches the corpus with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all the movies, lookup the entities in mapquest, and enrich the record for Solr\n",
    "def enrich_movies():\n",
    "    batches = []\n",
    "    tmdb_enriched = {}\n",
    "    \n",
    "    #Prep the batches for the mapquest API, and init the movies\n",
    "    for movie_id, movie in tmdbMovies(filename=\"../tmdb_spacy.json\"):\n",
    "        if \"location_entities\" in movie and movie[\"location_entities\"] and len(movie[\"location_entities\"]):\n",
    "            #Movie has GPE entities - add them to the mapquest batch\n",
    "            location_entities = movie[\"location_entities\"]\n",
    "            movie[\"location\"] = []\n",
    "            movie[\"location_city\"] = []\n",
    "            movie[\"location_state\"] = []\n",
    "            movie[\"location_country\"] = []\n",
    "            for entity in location_entities:\n",
    "                batches.append({\"id\":movie_id,\"location\":entity})\n",
    "        tmdb_enriched[movie_id]=movie\n",
    "\n",
    "    #Enrich all the movies in batches of 100, using the mapquest batch API\n",
    "    total = len(batches)\n",
    "    batch_step = 100\n",
    "    batch_nums = math.ceil(total/batch_step)\n",
    "    for i in range(0,batch_step*batch_nums,batch_step):\n",
    "        batch = batches[i:min(total,i+batch_step)]\n",
    "        geocoded = geocode_batch(batch)\n",
    "        for movie_id in geocoded:\n",
    "            for loc in geocoded[movie_id]:\n",
    "                csc = city_state_country(loc)\n",
    "                tmdb_enriched[movie_id][\"location\"].append(str(loc[\"latLng\"][\"lat\"]) + \",\" + str(loc[\"latLng\"][\"lng\"]))\n",
    "                if len(csc[\"City\"]):\n",
    "                    tmdb_enriched[movie_id][\"location_city\"].append(csc[\"City\"])\n",
    "                if len(csc[\"State\"]):\n",
    "                    tmdb_enriched[movie_id][\"location_state\"].append(csc[\"State\"])\n",
    "                if len(csc[\"Country\"]):\n",
    "                    tmdb_enriched[movie_id][\"location_country\"].append(csc[\"Country\"])\n",
    "\n",
    "    for movie_id in tmdb_enriched:\n",
    "        tmdb_enriched[movie_id][\"location\"] = dedup(tmdb_enriched[movie_id][\"location\"])\n",
    "        tmdb_enriched[movie_id][\"location_city\"] = dedup(tmdb_enriched[movie_id][\"location_city\"])\n",
    "        tmdb_enriched[movie_id][\"location_state\"] = dedup(tmdb_enriched[movie_id][\"location_state\"])\n",
    "        tmdb_enriched[movie_id][\"location_country\"] = dedup(tmdb_enriched[movie_id][\"location_country\"])\n",
    "        \n",
    "    writeTmdmMovies(tmdb_enriched,'../tmdb_enriched.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicates each movies location field data in the TMDB corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_movie_locations():\n",
    "    tmdb_enriched = {}\n",
    "    for movie_id, movie in tmdbMovies(filename=\"../tmdb_enriched.json\"):\n",
    "        if \"location\" in movie and len(movie[\"location\"]):\n",
    "            movie[\"location\"] = dedup(movie[\"location\"])\n",
    "            \n",
    "        if \"location_city\" in movie and len(movie[\"location_city\"]):\n",
    "            movie[\"location_city\"] = dedup(movie[\"location_city\"])\n",
    "            \n",
    "        if \"location_state\" in movie and len(movie[\"location_state\"]):\n",
    "            movie[\"location_state\"] = dedup(movie[\"location_state\"])\n",
    "            \n",
    "        if \"location_country\" in movie and len(movie[\"location_country\"]):\n",
    "            movie[\"location_country\"] = dedup(movie[\"location_country\"])\n",
    "            \n",
    "        tmdb_enriched[movie_id] = movie\n",
    "    writeTmdmMovies(tmdb_enriched,'../tmdb_enriched_deduped.json')\n",
    "dedup_movie_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and geocode the location entities in the corpus (done offline)\n",
    "#entitize_movies()\n",
    "#enrich_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tmdb_entities():\n",
    "    mc=0\n",
    "    lc=0\n",
    "    for movie in indexableMovies(filename='../tmdb_spacy.json'):\n",
    "        mc+=1\n",
    "        if \"location_entities\" in movie and len(movie[\"location_entities\"]):\n",
    "            lc+=1\n",
    "            print(\"-----------------\")\n",
    "            print(movie[\"title\"])\n",
    "            print(movie[\"location_entities\"])\n",
    "            \n",
    "def print_tmdb_locations():\n",
    "    mc=0\n",
    "    lc=0\n",
    "    for movie in indexableMovies(filename='../tmdb_enriched_deduped.json'):\n",
    "        mc+=1\n",
    "        if \"location\" in movie and len(movie[\"location\"]):\n",
    "            lc+=1\n",
    "            print(\"-----------------\")\n",
    "            print(movie[\"title\"])\n",
    "            print(movie[\"location\"])\n",
    "            print(movie[\"location_city\"])\n",
    "            print(movie[\"location_state\"])\n",
    "            print(movie[\"location_country\"]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
